
# Top 100 paper

# 解构

数据集 + 模型 + 训练 + 系统 + 工程 + 领域

Position Embedding
绝对位置编码
算法  |  rope
【通俗易懂-大模型的关键技术之一:旋转位置编码rope （3）】 https://www.bilibili.com/video/BV1Mj421R7JQ/?share_source=copy_web&vd_source=e0450010f297691cb0a94ff045e331a8
Normization
算法  |  Attention
Attention Is All You Need

Fast Transformer Decoding: One Write-Head is All You Need

GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints

DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model

FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness

FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning

FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision

Efficient Memory Management for Large Language Model Serving with PagedAttention

【Flash Attention 为什么那么快？原理讲解】 https://www.bilibili.com/video/BV1UT421k7rA/?share_source=copy_web&vd_source=e0450010f297691cb0a94ff045e331a8解决attention的稀疏性

为什么现在的LLM都是Decoder only的架构？ - Jennie666999的回答 - 知乎
https://www.zhihu.com/question/588325646/answer/2970989277

Rope
训练
benchmark
系统
工程
领域
AGI
大模型下 参数之多是足以过拟合的。。。可以吞噬掉所有的文档。。。看完全世界全部的视频。。。 整个世界都闭环了。。。
物理的尽头是数学结构  没想到语言知识的尽头也是数学结构
如果问到大模型是怎么部署的可以试一下刀刀宁的博客
【深度解读FAST'25最佳论文Mooncake】：存储为中心的LLM推理架构

# 研究方向
