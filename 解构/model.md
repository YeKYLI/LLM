# Position Embedding

## Rope

十分钟读懂旋转编码（RoPE） - 绝密伏击的文章 - 知乎
https://zhuanlan.zhihu.com/p/647109286

# Normalization

[以"从头开始实现Llama3"为线索，零基础入门大模型：词嵌入（embedding）及归一化（Normalization）篇](https://mp.weixin.qq.com/s/z7ukkqUc8VbI38_lKf8xGA)

Position Embedding
绝对位置编码
算法  |  rope
【通俗易懂-大模型的关键技术之一:旋转位置编码rope （3）】 https://www.bilibili.com/video/BV1Mj421R7JQ/?share_source=copy_web&vd_source=e0450010f297691cb0a94ff045e331a8
Normization
算法  |  Attention
Attention Is All You Need

Fast Transformer Decoding: One Write-Head is All You Need

GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints

DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model

FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness

FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning

FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision

Efficient Memory Management for Large Language Model Serving with PagedAttention

【Flash Attention 为什么那么快？原理讲解】 https://www.bilibili.com/video/BV1UT421k7rA/?share_source=copy_web&vd_source=e0450010f297691cb0a94ff045e331a8解决attention的稀疏性

为什么现在的LLM都是Decoder only的架构？ - Jennie666999的回答 - 知乎
https://www.zhihu.com/question/588325646/answer/2970989277

面试鹅厂，被FlashAttention虐的体无完肤... - 丁师兄大模型的文章 - 知乎
https://zhuanlan.zhihu.com/p/11273327848

Rope
